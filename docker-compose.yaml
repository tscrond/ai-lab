x-base_service: &base_service
    networks: 
      - ollama-network
    ports:
      - "${WEBUI_PORT:-7860}:7860"
    volumes:
      - &v1 ./data:/data
      - &v2 ./output:/output
    stop_signal: SIGKILL
    tty: true
    deploy:
      resources:
        reservations:
          devices:
              - driver: nvidia
                device_ids: ['0']
                capabilities: [compute, utility]

services:
  download:
    build: ./services/download/
    profiles: ["download"]
    volumes:
      - *v1

  auto: &automatic
    <<: *base_service
    container_name: auto
    profiles: ["auto"]
    build: ./services/AUTOMATIC1111
    image: sd-auto:78
    environment:
      - CLI_ARGS=--allow-code --medvram --xformers --enable-insecure-extension-access --api

  auto-cpu:
    <<: *automatic
    container_name: auto-cpu
    profiles: ["auto-cpu"]
    deploy: {}
    environment:
      - CLI_ARGS=--no-half --precision full --allow-code --enable-insecure-extension-access --api

  comfy: &comfy
    <<: *base_service
    container_name: comfy
    profiles: ["comfy"]
    build: ./services/comfy/
    image: sd-comfy:7
    environment:
      - CLI_ARGS=

  comfy-cpu:
    <<: *comfy
    container_name: comfy-cpu
    profiles: ["comfy-cpu"]
    deploy: {}
    environment:
      - CLI_ARGS=--cpu

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-models:/root/.ollama/models
    environment:
      - OLLAMA_ACCELERATOR=cuda
      - OLLAMA_USE_CUDA=1
      - NVIDIA_VISIBLE_DEVICES=all
      - GIN_MODE=release
      - OLLAMA_MODEL_DIR=/root/.ollama/models
      - OLLAMA_HOST=0.0.0.0
      - CUDA_MPS_ACTIVE_THREAD_PERCENTAGE=80
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 32GB
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ollama-network
    restart: unless-stopped

  open-webui:
    image: ghcr.io/open-webui/open-webui:v0.6.5
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      - 'DEFAULT_LOCALE=en'
      - 'ENABLE_OPENAI_API=False'
      - 'ENABLE_COMMUNITY_SHARING=False'
      - 'OLLAMA_BASE_URL=http://ollama:11434/'
      - 'WEBUI_SECRET_KEY='
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - ollama-network
    restart: unless-stopped

  caddy:
    container_name: caddy
    image: docker.io/library/caddy:2-alpine
    network_mode: host
    restart: unless-stopped
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy-data:/data:rw
      - caddy-config:/config:rw
    environment:
      - SEARXNG_TLS=${LETSENCRYPT_EMAIL}
        #cap_drop:
        #- ALL
    cap_add:
      - NET_BIND_SERVICE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  redis:
    container_name: redis
    image: docker.io/valkey/valkey:8-alpine
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    networks:
      - ollama-network
    volumes:
      - valkey-data2:/data
        #cap_drop:
        #- ALL
    cap_add:
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    networks:
      - ollama-network
    ports:
      - "0.0.0.0:8080:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      - SEARXNG_BASE_URL=https://${SEARXNG_HOSTNAME:-localhost}/
      - UWSGI_WORKERS=${SEARXNG_UWSGI_WORKERS:-4}
      - UWSGI_THREADS=${SEARXNG_UWSGI_THREADS:-4}
        #cap_drop:
        #- ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
    logging:
      driver: "json-file"
      options:
        max-size: "1m"
        max-file: "1"

networks:
  ollama-network:

volumes:
  caddy-data:
    driver: local
  caddy-config:
    driver: local
  valkey-data2:
    driver: local
  open-webui:
    driver: local
  ollama-models:
    driver: local
